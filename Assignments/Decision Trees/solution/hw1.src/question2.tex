
\section{Experiments}

This problem uses a modified version of the famous the Mushroom Dataset from the
UCI machine learning repository. Each data point has 21 features indicating
different characteristics of a mushroom. You can find definitions of each
feature in the file \texttt{information.txt}. The labels are either \texttt{p}
or \texttt{e}, which stand for poisonous and edible respectively.

Your task is to implement the ID3 algorithm and build a decision tree using the
training data \texttt{data/train.csv}. The goal of the decision tree is to
distinguish between poisonous and edible mushrooms. We also provide the test set
\texttt{data/test.csv} to evaluate how your decision tree classifier is
performing.


\paragraph{Programming notes.} You may use any programming language for your
implementation, but as discussed in class, we prefer Python. The graders should
be able to execute your code on the CADE machines without having to install any
libraries.  Remember that you are not allowed to use any machine learning
libraries (ex: \texttt{scikit-learn}, \texttt{tensorflow}, \texttt{pytorch},
etc.), but can use libraries with mathematical functions like \texttt{numpy} and
data management libraries like \texttt{pandas}.

We have provided a file \texttt{data.py}, which is code that can help you with
the data loading and manipulation if you choose to use Python. We have included
a Jupyter notebook \texttt{datademo.ipynb} that demonstrates how to use the file
and what functionality it provides. Using the code is not required, although we
think it should help you with the implementation of the algorithm. (If you are
familiar with a library such as \texttt{pandas}, you may find that to be helpful
instead of using the provided code.)

\subsection*{Cross-Validation}

The depth of the tree is a \emph{hyper-parameter} to the decision tree algorithm
that helps reduce overfitting. By depth, we refer to the maximum path length
from the root to any leaf. That is, a tree with just a single node has depth 0,
a tree with a root attribute directly leading to labels in one step has depth 1
and so on. You will see later in the semester that many machine learning
algorithm (SVM, logistic-regression, etc.) require choosing hyper-parameters
before training commences, and this choice can make a big difference in the
performance of the learners.  One way to determine a good hyper-parameter values
to use a technique called \emph{cross-validation}.

As usual, we have a training set and a test set. Our goal is to discover good
hyperparameters using the training set \emph{only}. Suppose we have a
hyperparameter (e.g. the depth of a decision tree) and we wish to ascertain
whether it is a good choice or not. To do so, we can set aside some of the
training data into a subset called the {\em validation} set and train on the
rest of the training data. When training is finished, we can test the resulting
classifier on the validation data. This allows us to get an idea of how well the
particular choice of hyper-parameters does.

However, since we did not train on the whole dataset, we may have introduced a
statistical bias in the classifier caused by the choice of the validation
set. To correct for this, we will need to repeat this process multiple times for
different choices of the validation set. That is, we need train many classifiers
with different subsets of the training data removed and average out the accuracy
across these trials.

For problems with small data sets, a popular method is the leave-one-out
approach. For each example, a classifier is trained on the rest of the data and
the chosen example is then evaluated. The performance of the classifier is the
average accuracy on all the examples.  The downside to this method is for a data
set with $n$ examples we must train $n$ different classifiers. Of course, this
is not practical in general, so we will hold out subsets of the data many times
instead.

Specifically, for this problem, you should implement $k$-fold cross validation.

The general approach for $k$-fold cross validation is the following: Suppose we
want to evaluate how good a particular hyper-parameter is. We randomly split the
training data into $k$ equal sized parts. Now, we will train the model on all
but one part with the chosen hyper-parameter and evaluate the trained model on
the remaining part. We should repeat this $k$ times, choosing a different part
for evaluation each time. This will give we $k$ values of accuracy.  Their
average cross-validation accuracy gives we an idea of how good this choice of
the hyper-parameter is. To find the best value of the hyper-parameter, we will
need to repeat this procedure for different choices of the hyper-parameter. Once
we find the best value of the hyper-parameter, we can use the value to retrain
we classifier using the entire training set.

\section*{Growing decision trees}

For this problem, your should use the data in \texttt{data} folder. This folder
contains two files: \texttt{train.csv} and \texttt{test.csv}. You should train
your algorithm on the training file.  Remember that you should not look at or
use your testing file until your training is complete.

\begin{enumerate}

\item\relax [6 points] \textbf{Baseline}

  First, find the most common label in the training data. What is the training
  and test accuracy of a classifier that always predicts this label?

\item \textbf{Implementation: Full trees}
  \label{impl:full}

  In the first set of decision tree experiments, run the ID3 algorithm we saw in
  class without any depth restrictions. (That is, there are no hyperparameters
  for this setting.)

  [6 points] Implement the decision tree data structure and the ID3 algorithm
  for your decision tree (Remember that the decision tree need not be a binary
  tree!). For debugging your implementation, you can use the previous toy
  examples like the toy data from Table 1. Discuss what approaches and design
  choices you had to make for your implementation and what data structures you
  used.  Also explain the organization of your code.

  Your report should include the following information
  \begin{enumerate}
  \item \relax[2 points] The root feature that is selected by your algorithm
  \item \relax[2 point] Information gain for the root feature
  \item \relax[2 points] Maximum depth of the tree that your implementation
    gives
  \item \relax[3 points] Accuracy on the training set
  \item \relax[5 points] Accuracy on the test set
  \end{enumerate}
  
\item \textbf{Limiting Depth}
  \label{impl:limiting}

  Next, you will perform 5-fold cross-validation to limit the depth of your
  decision tree, effectively pruning the tree to avoid overfitting. We have
  already randomly split the training data into five splits.  You should use the
  5 cross-validation files for this section, titled
  \texttt{data/CVfolds/foldX.csv} where \texttt{X} is a number between 1 and 5
  (inclusive).

  \begin{enumerate}
  \item \relax[15 points] Run 5-fold cross-validation using the specified
    files. Experiment with depths in the set $\{1, 2, 3, 4, 5, 10, 15\}$,
    reporting the average cross-validation accuracy and standard deviation for
    each depth. Explicitly specify which depth should be chosen as the best, and
    explain why. If a certain depth is not feasible for any reason, your report
    should explain why.
  \item \relax [4 points] Explain briefly how you implemented the depth limit
    functionality in your code.
  \item \relax[15 points] Using the depth with the greatest cross-validation
    accuracy from your experiments: train your decision tree on the
    \texttt{data/train.csv} file. Report the accuracy of your decision tree on the
    \texttt{data/test.csv} file.
  \item \relax[5 points] Discuss the performance of the depth limited tree as
    compared to the full decision tree. Do you think limiting depth is a good idea?
    Why?
  \end{enumerate}

\end{enumerate}

\subsection*{Experiment Submission Guidelines}
\begin{enumerate}
\item The report should detail your experiments. For each step, explain in no
  more than a paragraph or so how your implementation works. You may provide the
  results for the final step as a table or a graph.
\item \textit{Your code should run on the CADE machines}. You should include a
  shell script, \textbf{run.sh}, that will execute your code in the CADE
  environment. For each experiment, your code must print your results in the
  following order:
  \begin{enumerate}
  \item Entropy of the data
  \item Best feature and its information gain
  \item Cross-validation accuracies for each fold (for limiting depth setting)
  \item Best depth (for the limiting depth setting)
  \item Accuracy of the trained classifier on the training set (For the limiting
    depth setting, this would be for the tree with the best depth)
  \item Accuracy of the trained classifier on the test set (For the limiting
    depth setting, this would be for the tree with the best depth)
  \end{enumerate}

  Without these details, you will lose points for your implementation.
  
  You are responsible for ensuring that the grader can execute the code using
  only the included script. If you are using an esoteric programming language,
  you should make sure that its runtime is available on CADE.
\item Please do \textit{not} hand in binary files! We will not grade binary
  submissions.
\end{enumerate}


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
