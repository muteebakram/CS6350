\section{Decision Trees}
\label{sec:decision-trees}

\begin{enumerate}


\item~[25 points] 
  Mark loves mangoes. Unfortunately he is lousy at picking ripe mangoes at the grocery. He needs your help.
  You need to build a decision tree that will help Mark decide if a mango is ripe or not. You need to make the decision based on
  four features described below:
  \begin{enumerate}
  \item\textbf{Variety} (\textit{Alphonso, Keitt or Haden}): Describes the variety of the mango.
  \item\textbf{Color} (\textit{Red, Yellow or Green}): Describes the color of the mango.
  \item\textbf{Smell} (\textit{Sweet or None}): Describes the smell of the mango.
  \item\textbf{Time} (\textit{One or Two}): Number of weeks since the mango was plucked.
  \end{enumerate}

  You are given the following dataset which contains data for 8 different mangoes. For each mango, the values of the above four features
  are listed. The label of whether the mango was ripe or not is also provided.

  \begin{table}[h]
    \centering
    \begin{tabular}{llll|l}
      \hline
      Variety & Color    & Smell  & Time & Ripe?  \\ \hline
      Alphonso& Red      & None   & Two  & False  \\
      Keitt   & Red      & None   & One  & True   \\
      Alphonso& Yellow   & Sweet  & Two  & True   \\
      Keitt   & Green    & None   & Two  & False  \\
      Haden   & Green    & Sweet  & One  & True   \\
      Alphonso& Yellow   & None   & Two  & False  \\
      Keitt   & Yellow   & Sweet  & One  & False  \\
      Alphonso& Red      & Sweet  & Two  & True   \\ \hline
    \end{tabular}
    \caption{Training data for the mango prediction problem.}\label{tb-mango-train}
  \end{table}


  \begin{enumerate}
  \item~[5 points] How many possible functions are there to map these four features to a boolean decision? How many functions are consistent with the given training dataset?
  \item~[3 points] What is the entropy of the labels in this data? When calculating entropy, the base of the logarithm should be base 2.
  \item~[4 points] Compute the information gain of each feature and enter it
    into Table~\ref{tb-entropy-ig}. Specify upto 3 decimal places.
    \begin{table}[h]
      \centering
      \begin{tabular}{c|c}

        \hline
        Feature & Information Gain \\ \hline
        Variety &                  \\
        Color   &                  \\
        Smell   &                  \\
        Time    &                  \\ \hline
      \end{tabular}
      \caption{Information gain for each feature.}
      \label{tb-entropy-ig}
    \end{table}
  \item~[1 points] Which attribute will you use to construct the root of the
    tree using the information gain heuristic of the ID3 algorithm?
  \item~[8 points] Using the root that you selected in the previous question,
    construct a decision tree that represents the data. You do not have to use
    the ID3 algorithm here, you can show any tree with the chosen root.
  \item~[4 points] Suppose you are given three more examples, listed in
    Table~\ref{tb-mango-test}. Use your decision tree to predict the label for
    each example. Also report the accuracy of the classifier that you have
    learned.
  \end{enumerate}

  \begin{table}[h!]
    \centering
    \begin{tabular}{cccc|c}
      \hline
      Variety & Color  & Smell  & Time & Ripe?  \\ \hline
      Alphonso& Green  & Sweet  & Two  & True   \\
      Keitt   & Red    & Sweet  & One  & False  \\
      Haden   & Yellow & None   & Two  & True   \\ \hline
    \end{tabular}
    \caption{Test data for mango prediction problem}\label{tb-mango-test}
  \end{table}

\item~[10 points] Recall that in the ID3 algorithm, we want to identify the best attribute that splits the examples that are relatively pure in one label.
  Aside from entropy, which we saw in class and you used in the previous question, there are other methods to measure impurity.

  We will now develop a variant of the ID3 algorithm that does not use entropy. If, at some node, we stopped growing the tree and assign the most common label of the remaining examples at that node, then the empirical error on the training set at that node will be
  % 
  $$MajorityError = 1 - \max_{i}p_i$$
  % 
  where, $p_i$ is the fraction of examples that are labeled with the $i^{th}$ label.



  

  \begin{enumerate}
  \item~[2 points]   Notice that $MajorityError$ can be thought of as a measure of impurity just like entropy. Just like we used entropy to define information gain, we can define a new version of information gain that uses $MajorityError$ in place of entropy. Write down an expression that defines a new version of information gain that uses $MajorityError$ in place of entropy.
    
  \item~[6 points] Calculate the value of your newly defined information gain from the previous question for the four features in the mango dataset from ~\ref{tb-mango-train}. Use 3 significant digits. Enter the information gain into Table~\ref{tb-maj-ig}.
    
    \begin{table}[h!]
      \centering
      \begin{tabular}{c|c}
        \hline
        Feature & Information Gain (using majority error) \\ \hline
        Variety &                  \\
        Color   &                  \\
        Smell   &                  \\
        Time    &                  \\ \hline
      \end{tabular}
      \caption{Information gain for each feature.}\label{tb-maj-ig}
    \end{table}

  \item~[2 points] According to your results in the last question, which
    attribute should be the root for the decision tree?  Do these two measures
    (entropy and majority error) lead to the same tree?
  \end{enumerate}

\end{enumerate}



%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
